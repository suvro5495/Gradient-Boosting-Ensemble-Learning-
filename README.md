# Gradient-Boosting-Ensemble-Learning-
In the conducted machine learning experiment, we aimed to compare the performance of four gradient boosting algorithms: XGBoost, AdaBoost, LightGBM, and CatBoost, on the Iris dataset for multi-class classification.
I loaded the Iris dataset, split it into training and testing sets, and trained each algorithm individually. Upon training, we evaluated their performance using accuracy metrics and visualized the confusion matrices to assess classification results.
XGBoost, LightGBM, and CatBoost demonstrated high accuracy levels, indicating their proficiency in classifying Iris dataset samples. AdaBoost also showed respectable performance but slightly lower accuracy compared to the other algorithms. The confusion matrices provided insights into each algorithm's ability to correctly classify samples across different Iris species.
This experiment demonstrates the effectiveness of gradient boosting algorithms in handling multi-class classification tasks, especially in the context of the Iris dataset. The visualization of confusion matrices helps interpret the algorithms' classification behavior and identify potential areas for improvement. The results can serve as a valuable reference for practitioners and researchers working with gradient boosting techniques in various classification tasks.
